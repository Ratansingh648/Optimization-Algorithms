{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "#### Author: Ratan Madankumar Singh                                                                                                                              \n",
    "#### 16th February 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I have implemented few most commonly used cost functions used for regression and corresponding parameter updating algorithms. We will check the effect of each of these algorithms on a particular dataset to have a comparison between the optimal value and rate of convergence of these algorithms. Here I have implemented \n",
    "\n",
    "- **Batch Gradient Descent**\n",
    "- **Gradient Descent with Momentum**\n",
    "- **RMS Propagation**\n",
    "- **Adam**\n",
    "\n",
    "These algorithms have their parameter initialization functions as well. We will check the rate of convergence using an example of Single perceptron model applied to a dataset of credit card fraud detection. \n",
    "\n",
    "### implementation of Single Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_hat = 1.0/(1+np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "def weightInit(n):\n",
    "    W = np.random.random((1,n))\n",
    "    b = 0\n",
    "    return [W,b]\n",
    "\n",
    "def forwardPropagation(W,X,b):\n",
    "    z = np.dot(W,X)+b\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat\n",
    "\n",
    "def computeCost(y_hat,y,lambd,W):\n",
    "    m = len(np.squeeze(y_hat))\n",
    "    J = np.sum((y_hat-y)**2) + lambd*np.sum(np.array(W)**2)\n",
    "    return J\n",
    "\n",
    "def computeGradient(y_hat,y,X,lambd,W):\n",
    "    m = len(np.squeeze(y_hat))\n",
    "    dW = np.dot((y_hat-y),X.T)/m + lambd*np.array(W)/m\n",
    "    db = np.sum(y_hat-y)/m\n",
    "    return [dW,db]\n",
    "\n",
    "def predict(W,b,X):\n",
    "    z = np.dot(W,X)+b\n",
    "    y_hat = sigmoid(z)\n",
    "    y_hat = (y_hat > 0.5)*1.0\n",
    "    return y_hat\n",
    "\n",
    "def accuracy(y_hat,y):\n",
    "    acc = np.sum(y_hat == y)*100.0/len(np.squeeze(y))\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Optimization Algorithms\n",
    "Now we are done with the writing the nitty gritty of code needed for our single Perceptron model to implement. Now we can implement our optimization algorithms actually. These optimization algorithms require some other hyperparameters as well which we need to initialize seperately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "def gradientDescent(W,b,dW,db,alpha):\n",
    "    W = W - alpha*dW\n",
    "    b = b - alpha*db\n",
    "    return [W,b]\n",
    "\n",
    "# Gradient Descent with Momentum\n",
    "\n",
    "def initMomentum(W,b):\n",
    "    Vw = np.zeros(W.shape)\n",
    "    Vb = 0\n",
    "    return [Vw,Vb]\n",
    "\n",
    "def gradientDescentMomentum(W,b,dW,db,Vw,Vb,beta,alpha):\n",
    "    Vw = beta*Vw + (1-beta)*dW\n",
    "    Vb = beta*Vb + (1-beta)*db\n",
    "    W = W - alpha*Vw\n",
    "    b = b - alpha*Vb\n",
    "    return [W,b,Vw,Vb]\n",
    "\n",
    "# RMS Propagation\n",
    "\n",
    "def initRMS(W,b):\n",
    "    Sw = np.zeros(W.shape)\n",
    "    Sb = 0\n",
    "    return [Sw,Sb]\n",
    "\n",
    "def RMSProp(W,b,dW,db,Sw,Sb,beta,alpha,epsilon = 1e-8):\n",
    "    Sw = beta*Sw + (1-beta)*np.square(dW)\n",
    "    Sb = beta*Sb + (1-beta)*np.square(db)\n",
    "    W = W - alpha*dW/(np.sqrt(Sw)+epsilon)\n",
    "    b = b - alpha*db/(np.sqrt(Sb)+epsilon)\n",
    "    return [W,b,Sw,Sb]\n",
    "\n",
    "# Adam\n",
    "\n",
    "def initAdam(W,b):\n",
    "    Sw = np.zeros(W.shape)\n",
    "    Sb = 0\n",
    "    Vw = np.zeros(W.shape)\n",
    "    Vb = 0\n",
    "    return [Sw,Sb,Vw,Vb]\n",
    "\n",
    "def adam(W,b,dW,db,Sw,Sb,Vw,Vb,beta1,beta2,alpha,epsilon = 1e-8):\n",
    "    Sw = beta1*Sw + (1-beta1)*np.square(dW)\n",
    "    Sb = beta1*Sb + (1-beta1)*np.square(db)\n",
    "    Vw = beta2*Vw + (1-beta2)*dW\n",
    "    Vb = beta2*Vb + (1-beta2)*db\n",
    "    W = W - alpha*Vw/(np.sqrt(Sw)+epsilon)\n",
    "    b = b - alpha*Vb/(np.sqrt(Sb)+epsilon)\n",
    "    return [W,b,Sw,Sb,Vw,Vb]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking our optimization Algorithm Performance\n",
    "Now we will load our dataset. This dataset has been taken from kaggle and contains different parameters to detect a credit card fraud detection. The fraudulent transactions has been labeled as '1' and rest as '0'. Since we are just observing performance of our algorithms, we will not discuss our dataset in detail. I have stored dataset in my local machine. Before applying machine learning algorithm, we will split dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of file is 284807 rows and 30 columns\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:\\\\Users\\\\Ratan Singh\\\\Documents\\\\Neural Network\\\\Optimization Algorithms\\\\creditcard.csv\"\n",
    "OriginalDataFrame = pd.read_csv(file_path)\n",
    "print(\"Dimensions of file is \"+str(DataFrame.shape[0])+\" rows and \"+str(DataFrame.shape[1])+\" columns\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since we don't need a column \"Time\" so we will drop it from the dataset. Also we are splitting our data into trainData and testData with 200000 rows in trainData. Before spliting we are randomizing the data so that both trainData and testData comes from the same distribution and our model is not affected by bias. Also here we are setting some more hyperparameters like **number of iterations (num_iter)**, **learning rate (alpha) ** , **Regularization Parameter (lambd)**  and setting the basic flow of learning. To store the results of different optimization algorithm, we initialize different cost functions, different parameters and different gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of file is 284807 rows and 30 columns\n",
      "ITERATION::0\n",
      "ITERATION::100\n",
      "ITERATION::200\n",
      "ITERATION::300\n",
      "ITERATION::400\n",
      "ITERATION::500\n",
      "ITERATION::600\n",
      "ITERATION::700\n",
      "ITERATION::800\n",
      "ITERATION::900\n",
      "ITERATION::1000\n",
      "ITERATION::1100\n",
      "ITERATION::1200\n",
      "ITERATION::1300\n",
      "ITERATION::1400\n",
      "Training the model is completed ::\n"
     ]
    }
   ],
   "source": [
    "DataFrame = OriginalDataFrame.drop([\"Time\"],axis = 1)\n",
    "order = np.random.permutation(DataFrame.shape[0])\n",
    "print(\"Dimensions of file is \"+str(DataFrame.shape[0])+\" rows and \"+str(DataFrame.shape[1])+\" columns\" )\n",
    "\n",
    "DataFrame = DataFrame.iloc[order,:]\n",
    "\n",
    "cutOff = 200000\n",
    "\n",
    "X_trainData = DataFrame.iloc[range(0,cutOff),range(0,29)].T\n",
    "X_testData = DataFrame.iloc[range(cutOff,DataFrame.shape[0]),range(0,29)].T\n",
    "\n",
    "Y_trainData = (DataFrame.iloc[range(0,cutOff),29].T).values.reshape(1,cutOff)\n",
    "Y_testData = (DataFrame.iloc[range(cutOff,DataFrame.shape[0]),29].T).values.reshape(1,DataFrame.shape[0]-cutOff)\n",
    "\n",
    "num_iter = 1500\n",
    "alpha = 0.1\n",
    "lambd = 0.1\n",
    "beta = 0.9\n",
    "beta1 = 0.99\n",
    "beta2 = 0.9\n",
    "\n",
    "J_bgd = []\n",
    "J_gdm = []\n",
    "J_RMS = []\n",
    "J_adm = []\n",
    "\n",
    "[W_init,b_init] = weightInit(29)\n",
    "\n",
    "[W_bgd,b_bgd] = [W_init,b_init]\n",
    "[W_gdm,b_gdm] = [W_init,b_init]\n",
    "[W_RMS,b_RMS] = [W_init,b_init]\n",
    "[W_adm,b_adm] = [W_init,b_init]\n",
    "\n",
    "[Vw_gdm,Vb_gdm] = initMomentum(W_gdm,b_gdm)\n",
    "[Sw_RMS,Sb_RMS] = initRMS(W_RMS,b_RMS)\n",
    "[Sw_adm,Sb_adm,Vw_adm,Vb_adm] = initAdam(W_adm,b_adm)\n",
    "\n",
    "\n",
    "for i in range(0,num_iter):\n",
    "    \n",
    "    y_hat_bgd = forwardPropagation(W_bgd,X_trainData,b_bgd)\n",
    "    y_hat_gdm = forwardPropagation(W_gdm,X_trainData,b_gdm)\n",
    "    y_hat_RMS = forwardPropagation(W_RMS,X_trainData,b_RMS)\n",
    "    y_hat_adm = forwardPropagation(W_adm,X_trainData,b_adm)\n",
    "    \n",
    "    if(i%100 == 0):\n",
    "        print(\"ITERATION::\"+str(i))\n",
    "        J_bgd.append(computeCost(y_hat_bgd,Y_trainData,lambd,W_bgd))\n",
    "        J_gdm.append(computeCost(y_hat_gdm,Y_trainData,lambd,W_gdm))\n",
    "        J_RMS.append(computeCost(y_hat_RMS,Y_trainData,lambd,W_RMS))\n",
    "        J_adm.append(computeCost(y_hat_adm,Y_trainData,lambd,W_adm))\n",
    "    \n",
    "    [dW_bgd,db_bgd] = computeGradient(y_hat_bgd,Y_trainData,X_trainData,lambd,W_bgd)\n",
    "    [dW_gdm,db_gdm] = computeGradient(y_hat_gdm,Y_trainData,X_trainData,lambd,W_gdm)\n",
    "    [dW_RMS,db_RMS] = computeGradient(y_hat_RMS,Y_trainData,X_trainData,lambd,W_RMS)\n",
    "    [dW_adm,db_adm] = computeGradient(y_hat_adm,Y_trainData,X_trainData,lambd,W_adm)\n",
    "    \n",
    "    [W_bgd,b_bgd] = gradientDescent(W_bgd,b_bgd,dW_bgd,b_bgd,alpha)\n",
    "    [W_gdm,b_gdm,Vw_gdm,Vb_gdm] = gradientDescentMomentum(W_gdm,b_gdm,dW_gdm,db_gdm,Vw_gdm,Vb_gdm,beta,alpha)\n",
    "    [W_RMS,b_RMS,Sw_RMS,Sb_RMS] = RMSProp(W_RMS,b_RMS,dW_RMS,db_RMS,Sw_RMS,Sb_RMS,beta,alpha)\n",
    "    [W_adm,b_adm,Sw_adm,Sb_adm,Vw_adm,Vb_adm] = adam(W_adm,b_adm,dW_adm,db_adm,Sw_adm,Sb_adm,Vw_adm,Vb_adm,beta1,beta2,alpha)\n",
    "        \n",
    "print(\"Training the model is completed ::\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------------+-----------------+---------------+\n",
      "| Gradient Descent | Gradient Descent Momentum | RMS Propagation |      Adam     |\n",
      "+------------------+---------------------------+-----------------+---------------+\n",
      "|  182854.335881   |       182854.335881       |  182854.335881  | 182854.335881 |\n",
      "|  1449.83039971   |       1219.86143036       |  199.629982152  | 214.405246009 |\n",
      "|  1649.49566123   |       1132.61488642       |  3276.16160607  | 217.377575734 |\n",
      "|  2725.20680498   |       1055.22523983       |  180.663596288  | 220.254575133 |\n",
      "|  5998.32547266   |       979.504068281       |   175.34524058  | 209.156665238 |\n",
      "|  5148.27747178   |       904.055347839       |  174.238366628  | 197.767441804 |\n",
      "|  3948.53477997   |       924.738043288       |  186.619005844  | 192.349844329 |\n",
      "|  3223.39882396   |       1283.37727125       |  132.116528537  | 164.705225425 |\n",
      "|  2789.79126546   |       2787.39988929       |  177.380202071  | 165.095513588 |\n",
      "|  2502.22336152   |       1979.25750686       |  179.890032585  | 177.948017456 |\n",
      "|  2282.00109106   |       1018.71991066       |  183.255329884  | 202.686748921 |\n",
      "|  2097.21380959   |        655.98684049       |   183.07835575  | 186.309438292 |\n",
      "|  1932.81328808   |       481.294781543       |  165.364988691  | 201.286435436 |\n",
      "|   1793.1975277   |       386.670973353       |  152.521061594  | 148.642597464 |\n",
      "|  1672.41160516   |       328.376038044       |  178.404587026  | 175.122015041 |\n",
      "+------------------+---------------------------+-----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "table = PrettyTable()\n",
    "table.add_column('Gradient Descent',J_bgd)\n",
    "table.add_column('Gradient Descent Momentum',J_gdm)\n",
    "table.add_column('RMS Propagation',J_RMS)\n",
    "table.add_column('Adam',J_adm)\n",
    "print(table)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy due to gradient Descent :: 99.6120603252\n",
      "Accuracy due to gradient Descent with Momentum :: 99.8891600929\n",
      "Accuracy due to RMS Propagation :: 99.9127430519\n",
      "Accuracy due to Adam :: 99.9068473121\n"
     ]
    }
   ],
   "source": [
    "y_hat_bgd = predict(W_bgd,b_bgd,X_testData)\n",
    "y_hat_gdm = predict(W_gdm,b_gdm,X_testData)\n",
    "y_hat_RMS = predict(W_RMS,b_RMS,X_testData)\n",
    "y_hat_adm = predict(W_adm,b_adm,X_testData)\n",
    "\n",
    "print(\"Accuracy due to gradient Descent :: \"+str(accuracy(y_hat_bgd,Y_testData)))\n",
    "print(\"Accuracy due to gradient Descent with Momentum :: \"+str(accuracy(y_hat_gdm,Y_testData)))\n",
    "print(\"Accuracy due to RMS Propagation :: \"+str(accuracy(y_hat_RMS,Y_testData)))\n",
    "print(\"Accuracy due to Adam :: \"+str(accuracy(y_hat_adm,Y_testData)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
